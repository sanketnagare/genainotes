## üß† What is Attention?

> In deep learning, **attention** helps a model decide **which words in a sentence are important to focus on** while making a decision.

Imagine you read the sentence:  
üëâ ‚ÄúThe cat sat on the mat because **it** was warm.‚Äù

To understand what "**it**" refers to (mat or cat?), your brain pays attention to certain earlier words. This is what **self-attention** does!

## ‚úçÔ∏è Sentence Example:
![](/images/Pasted image 20250610093537.png)

**"Hi, how are you"**

This sentence is used throughout the image to show how self-attention works.

## üî∂ PART 1: **Embedding**

### üî∏ What happens:

- Every word like **"Hi"**, **"How"**, etc. gets converted into a **vector of numbers**.  
    This is done using **word embeddings** like **Word2Vec**, **GloVe**, or **learned embeddings**.

üìå For example:
```
"Hi" ‚Üí [0.1, 0.2, ..., 512 values] ‚Üí (Size = 512)
```

That‚Äôs why you see:
```
Hi = x1 ‚Üí [........] ‚Üí 512
How = x2 ‚Üí [........] ‚Üí 512
Are = x3 ‚Üí [........] ‚Üí 512
You = x4 ‚Üí [........] ‚Üí 512
```

These are now **input vectors** (denoted as X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, X‚ÇÑ).

## üî∂ PART 2: **Self-Attention Mechanism**

### ü§î What is self-attention?

> Self-attention **compares each word with every other word** in the sentence to decide how much focus should be given to each one.

So "You" might pay attention to "Are", "How", and "Hi" ‚Äî but maybe more to "Are".

### üß± 3 important components are created from the input:

- **Q (Query)**
- **K (Key)**
- **V (Value)**

Each word is transformed into these three using learnable matrices.

## üî∂ PART 3: Dot Product ‚Üí Attention Score

You now compute attention scores by:
```
Score = Query ‚Ä¢ Key·µÄ
```

This forms a matrix of how much **each word pays attention to every other word**.
In the image, this is the **box on the top-right** with X‚ÇÅ, X‚ÇÇ, X‚ÇÉ.
Then, the **scores are normalized** (softmax) so they all sum to 1.

## üî∂ PART 4: Final Output Vectors

The attention weights are multiplied with **V (Values)** to get the final output vectors.

So the output for "Hi" is:

```
y‚ÇÅ = w‚ÇÅ‚Ä¢V‚ÇÅ + w‚ÇÇ‚Ä¢V‚ÇÇ + w‚ÇÉ‚Ä¢V‚ÇÉ + ...
```

Each word gets a new representation that **captures the context** around it.

## üìå When to Use Self-Attention?

Use when:

- You want a model to understand context in **text**, **code**, **music**, etc.
- You‚Äôre building:
    - **Chatbots**
    - **Language models** (GPT, BERT)
    - **Text summarizers**
    - **Translation tools**

## üß† Why Is Self-Attention Better Than RNN?

|RNN|Attention|
|---|---|
|Reads words one-by-one|Looks at all words at once|
|Slow|Fast (parallel)|
|Can forget long-term info|Captures long dependencies easily|
