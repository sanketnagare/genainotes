# ğŸ§  What are Transformers?

Transformers are models that **read and understand sequences of words** (like text), images, or even code, by focusing on the **most relevant parts** using something called **self-attention**.

They were introduced in **2017 (paper: â€œAttention is All You Needâ€)** and used in:

- ChatGPT
- Google Translate
- BERT
- DALLÂ·E, and more.

## ğŸ”¶ Image 1: High-Level View of Transformers

![](/images/Pasted image 20250610111337.png)


![](/images/Pasted image 20250610111418.png)
### â¤ **1. Input â†’ Encoder â†’ Decoder â†’ Output**

- **Input**: For example, an English sentence like â€œHow are you?â€
- **Encoder**: Reads the full input sentence and understands it.
- **Decoder**: Uses this understanding to generate something â€” like the **translated version** in another language.

> Example:  
> English Input â†’ Encoder  
> Russian Output â† Decoder


### â¤ **2. Blocks (Enc1, Enc2, â€¦ Dec1, Dec2, â€¦)**

These are **stacked layers** inside encoders and decoders.

- More layers â†’ more understanding
- GPT-2 is shown as only decoder layers (used in text generation).

â¤ **3. Use Case Example**:

```
Input: â€œHow are you?â€ (in English)  
Transformer â†’  
Output: â€œĞšĞ°Ğº Ğ´ĞµĞ»Ğ°?â€ (in Russian)
```

## ğŸ”¶ Image 2: The Core Transformer Architecture (with color diagram)

Letâ€™s now go deep into the diagram:

**Tokenization**  
Converts sentence:
```
â€œHi, How Are Youâ€
```

into small chunks like:

```
[â€œHiâ€, â€œ,â€, â€œHowâ€, â€œAreâ€, â€œYouâ€]
```

**Vocabulary Indexing**  
Every word/token is given a number:

```
â€œHiâ€ â†’ 1  
â€œHowâ€ â†’ 10  
â€œAreâ€ â†’ 30
```

1. **Normalization**  
    Ensures all values are within a certain range â€” helps training.
    
2. **Embedding Layer**  
    Converts word numbers into vectors (with 512 dimensions, for example), so machine can understand their **meaning**.

### ğŸŸ¦ **Positional Encoding**

Words lose position in embeddings â€” so this adds information like:

- â€œHiâ€ is 1st
- â€œAreâ€ is 3rd  
    ... so the model understands **word order**.

### ğŸ”µ **Encoder Stack (Left Block)**

Each Encoder Layer has:

- **Multi-Head Attention** (focuses on different parts of sentence)
- **Add & Normalize**
- **Feed-Forward Network (FFN)**

ğŸ§  The encoder reads the sentence and makes a smart representation of it.


### ğŸŸ£ **Decoder Stack (Right Block)**

Each Decoder Layer has:

- **Masked Multi-Head Attention** â†’ prevents cheating by blocking future words.
- **Encoder-Decoder Attention** â†’ learns from encoder's output.
- **Feed-Forward Layer**

Then â†’ Output generated word-by-word (e.g., one word at a time in translation).

ğŸ§± Summary: Full Transformer Flow

```
Input Sentence
   â†“
Tokenizer + Embedding + Positional Encoding
   â†“
Encoder Stack (Self Attention + FFN)
   â†“
Decoder Stack (Attention + Encoder context)
   â†“
Softmax â†’ Output word prediction (e.g., Russian word)
```

## ğŸ’¡When to Use Transformers

| Use Case                        | Why Transformer Works Best             |
| ------------------------------- | -------------------------------------- |
| **Language Translation**        | Captures long-range word relationships |
| **Chatbots (ChatGPT)**          | Uses both context + prediction         |
| **Text Summarization**          | Focuses on main points                 |
| **Image Captioning (with ViT)** | Adapts transformer for image patches   |
| **Code Completion (Codex)**     | Handles long code history + structure  |

## ğŸ§  Memory Trick

Remember **Transformers = E+D+A**

- **E**ncoder â†’ Understand input
- **D**ecoder â†’ Generate output
- **A**ttention â†’ Focus on important words





