## 1. 🟡 Fully Connected Neural Network (FNN)

![](/images/Pasted image 20250609192114.png)

Also called **Dense Network** or **Multi-Layer Perceptron (MLP)**
### 🔍 What it does:

- Every neuron is connected to every neuron in the next layer.
- Learns using **backpropagation**.
- Best for structured data.

### ✅ Use for:

- Classification (e.g., spam or not)
- Regression (e.g., house price)
- Function approximation (math equations, forecasting)

📌 **When to use**: When your data is **non-image/tabular** (Excel-like data).


## 2. 🟠 Convolutional Neural Network (CNN)

![](/images/Pasted image 20250609192309.png)

The **superstar of image processing**
### 🔍 What it does:

- Works like a **camera filter** that scans small portions (patches) of an image.
- Converts large images (e.g., 1080x1080) into important features (edges, curves).
- Layers: Convolution → ReLU → Pooling → Fully Connected

### ✅ Use for:

- Image classification (cats vs dogs)
- Face detection
- Video recognition
- Text (using 1D CNN for sentence-level tasks)

📌 **When to use**: When your input is **images, videos, or grid-like data**.

## 3. 🔁 Recurrent Neural Network (RNN)

![](/images/Pasted image 20250609192546.png)

RNN has **memory** of past input.
### 🔍 What it does:

- It loops back the output to the input → remembers past time steps.
- Works well for **sequence prediction**.

### ✅ Use for:

- Time-series forecasting
- Sentiment analysis
- Speech recognition
- Text generation

📌 **When to use**: When your input is **ordered/sequential**, like text, music, or weather over time.

## 4. 🧠 GAN (Generative Adversarial Network)

![](/images/Pasted image 20250609192735.png)

GAN = 2 neural networks fighting like a game 🎮

### 🔍 What it does:

- **Generator (Gen)**: creates fake samples
- **Discriminator (Dis)**: tries to catch fakes
- Trains until fake data becomes indistinguishable from real

### ✅ Use for:

- Image generation (e.g., fake faces, art)
- Deepfake videos
- Synthetic medical images

📌 **When to use**: When you want to **create new data** or enhance/restore it.

## 5. 🎯 Attention Model

![](/images/Pasted image 20250609192947.png)

Used in **NLP (Natural Language Processing)**

### 🔍 What it does:

- Focuses on **important parts of input**.
- Instead of processing input equally, it “pays attention” to relevant tokens.

### ✅ Use for:

- Machine translation (English to French)
- Text summarization
- Chatbots

📌 **When to use**: In language models or when **context matters** in sequences.

## 6. 🔁 Autoencoders


![](/images/Pasted image 20250609193158.png)

Used for **data compression & reconstruction**

### 🔍 What it does:

- Learns to **compress input into a short code**, and **reconstruct** the same back.
- Useful when you need to **reduce dimensionality**.

### ✅ Use for:

- Noise removal from images
- Anomaly detection (if reconstruction error is high → anomaly)
- Dimensionality reduction (like PCA but neural-style)

📌 **When to use**: When you need **compression, denoising, or anomaly detection**.


## 🔁 LSTM – Long Short-Term Memory

### 🌟 What is it?

LSTM is a **special type of Recurrent Neural Network (RNN)** that is designed to **remember information for long periods**. While RNNs struggle with long sentences or time sequences, LSTM fixes this using **memory cells** and **gates**.

![](/images/Pasted image 20250609193624.png)

### 🧠 Why is it special?

In a normal RNN, when you try to learn:

> "Sanket went to Pune. He took the train. It was comfortable."

The word **"It"** should refer to the **train** — but regular RNN forgets earlier words.

**LSTM remembers "train"** even after a few steps! ✅

### 🧩 How LSTM works (in simple blocks)

LSTM has 3 special **gates**:

|Gate|What it does|Analogy|
|---|---|---|
|🟢 **Input Gate**|Decides what new info to store|"What’s important to remember now?"|
|🔴 **Forget Gate**|Decides what old info to delete|"What’s no longer useful?"|
|🔵 **Output Gate**|Decides what to send to the next layer|"What should I output right now?"|

And there's a **memory cell** 🧠 that holds the long-term knowledge.



## ⚡ Transformers

### 🌟 What is it?

Transformers are **next-gen neural networks** for handling sequences **without recursion or memory loops** like RNNs.

They use **attention mechanisms** to decide what parts of the input are most important — **like humans!** 🔍

### 💡 Why Transformers?

RNN and LSTM read text word-by-word.

Transformers **look at the whole sentence at once**, and learn what words are related — even far away!

🧠 "The train 🚆 that I took from Mumbai to Delhi was fast."  
It figures out that “train” is the subject of “was fast” — even though other words are in between.

### ⚙️ Key Components of Transformer

| Component                  | Meaning                                                  |
| -------------------------- | -------------------------------------------------------- |
| 🎯 **Self-Attention**      | Helps each word focus on other relevant words            |
| 🧱 **Encoder**             | Reads and understands the input                          |
| 📤 **Decoder**             | Generates output (used in translation, text generation)  |
| 🔀 **Positional Encoding** | Adds word position info (since there's no loop like RNN) |
### ✅ When to Use Transformers

|Task|Use Case|
|---|---|
|🌐 Language translation|English → Hindi, German → French|
|💬 Chatbots|GPT, BERT, LLaMA all use transformers|
|📝 Text summarization|Converts big documents to highlights|
|🎥 Video captioning|Adds text to videos by understanding sequences|

### 🤖 Famous Models Based on Transformers:

| Model                    | Use                                            |
| ------------------------ | ---------------------------------------------- |
| BERT                     | Question-answering, Google search              |
| GPT (ChatGPT)            | Text generation                                |
| T5                       | Text-to-text tasks                             |
| Vision Transformer (ViT) | For image recognition using transformer blocks |








