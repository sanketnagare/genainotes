## 1. ğŸŸ¡ Fully Connected Neural Network (FNN)

![](/images/Pasted image 20250609192114.png)

Also called **Dense Network** or **Multi-Layer Perceptron (MLP)**
### ğŸ” What it does:

- Every neuron is connected to every neuron in the next layer.
- Learns using **backpropagation**.
- Best for structured data.

### âœ… Use for:

- Classification (e.g., spam or not)
- Regression (e.g., house price)
- Function approximation (math equations, forecasting)

ğŸ“Œ **When to use**: When your data is **non-image/tabular** (Excel-like data).


## 2. ğŸŸ  Convolutional Neural Network (CNN)

![](/images/Pasted image 20250609192309.png)

The **superstar of image processing**
### ğŸ” What it does:

- Works like a **camera filter** that scans small portions (patches) of an image.
- Converts large images (e.g., 1080x1080) into important features (edges, curves).
- Layers: Convolution â†’ ReLU â†’ Pooling â†’ Fully Connected

### âœ… Use for:

- Image classification (cats vs dogs)
- Face detection
- Video recognition
- Text (using 1D CNN for sentence-level tasks)

ğŸ“Œ **When to use**: When your input is **images, videos, or grid-like data**.

## 3. ğŸ” Recurrent Neural Network (RNN)

![](/images/Pasted image 20250609192546.png)

RNN has **memory** of past input.
### ğŸ” What it does:

- It loops back the output to the input â†’ remembers past time steps.
- Works well for **sequence prediction**.

### âœ… Use for:

- Time-series forecasting
- Sentiment analysis
- Speech recognition
- Text generation

ğŸ“Œ **When to use**: When your input is **ordered/sequential**, like text, music, or weather over time.

## 4. ğŸ§  GAN (Generative Adversarial Network)

![](/images/Pasted image 20250609192735.png)

GAN = 2 neural networks fighting like a game ğŸ®

### ğŸ” What it does:

- **Generator (Gen)**: creates fake samples
- **Discriminator (Dis)**: tries to catch fakes
- Trains until fake data becomes indistinguishable from real

### âœ… Use for:

- Image generation (e.g., fake faces, art)
- Deepfake videos
- Synthetic medical images

ğŸ“Œ **When to use**: When you want to **create new data** or enhance/restore it.

## 5. ğŸ¯ Attention Model

![](/images/Pasted image 20250609192947.png)

Used in **NLP (Natural Language Processing)**

### ğŸ” What it does:

- Focuses on **important parts of input**.
- Instead of processing input equally, it â€œpays attentionâ€ to relevant tokens.

### âœ… Use for:

- Machine translation (English to French)
- Text summarization
- Chatbots

ğŸ“Œ **When to use**: In language models or when **context matters** in sequences.

## 6. ğŸ” Autoencoders


![](/images/Pasted image 20250609193158.png)

Used for **data compression & reconstruction**

### ğŸ” What it does:

- Learns to **compress input into a short code**, and **reconstruct** the same back.
- Useful when you need to **reduce dimensionality**.

### âœ… Use for:

- Noise removal from images
- Anomaly detection (if reconstruction error is high â†’ anomaly)
- Dimensionality reduction (like PCA but neural-style)

ğŸ“Œ **When to use**: When you need **compression, denoising, or anomaly detection**.


## ğŸ” LSTM â€“ Long Short-Term Memory

### ğŸŒŸ What is it?

LSTM is a **special type of Recurrent Neural Network (RNN)** that is designed to **remember information for long periods**. While RNNs struggle with long sentences or time sequences, LSTM fixes this using **memory cells** and **gates**.

![](/images/Pasted image 20250609193624.png)

### ğŸ§  Why is it special?

In a normal RNN, when you try to learn:

> "Sanket went to Pune. He took the train. It was comfortable."

The word **"It"** should refer to the **train** â€” but regular RNN forgets earlier words.

**LSTM remembers "train"** even after a few steps! âœ…

### ğŸ§© How LSTM works (in simple blocks)

LSTM has 3 special **gates**:

|Gate|What it does|Analogy|
|---|---|---|
|ğŸŸ¢ **Input Gate**|Decides what new info to store|"Whatâ€™s important to remember now?"|
|ğŸ”´ **Forget Gate**|Decides what old info to delete|"Whatâ€™s no longer useful?"|
|ğŸ”µ **Output Gate**|Decides what to send to the next layer|"What should I output right now?"|

And there's a **memory cell** ğŸ§  that holds the long-term knowledge.



## âš¡ Transformers

### ğŸŒŸ What is it?

Transformers are **next-gen neural networks** for handling sequences **without recursion or memory loops** like RNNs.

They use **attention mechanisms** to decide what parts of the input are most important â€” **like humans!** ğŸ”

### ğŸ’¡ Why Transformers?

RNN and LSTM read text word-by-word.

Transformers **look at the whole sentence at once**, and learn what words are related â€” even far away!

ğŸ§  "The train ğŸš† that I took from Mumbai to Delhi was fast."  
It figures out that â€œtrainâ€ is the subject of â€œwas fastâ€ â€” even though other words are in between.

### âš™ï¸ Key Components of Transformer

| Component                  | Meaning                                                  |
| -------------------------- | -------------------------------------------------------- |
| ğŸ¯ **Self-Attention**      | Helps each word focus on other relevant words            |
| ğŸ§± **Encoder**             | Reads and understands the input                          |
| ğŸ“¤ **Decoder**             | Generates output (used in translation, text generation)  |
| ğŸ”€ **Positional Encoding** | Adds word position info (since there's no loop like RNN) |
### âœ… When to Use Transformers

|Task|Use Case|
|---|---|
|ğŸŒ Language translation|English â†’ Hindi, German â†’ French|
|ğŸ’¬ Chatbots|GPT, BERT, LLaMA all use transformers|
|ğŸ“ Text summarization|Converts big documents to highlights|
|ğŸ¥ Video captioning|Adds text to videos by understanding sequences|

### ğŸ¤– Famous Models Based on Transformers:

| Model                    | Use                                            |
| ------------------------ | ---------------------------------------------- |
| BERT                     | Question-answering, Google search              |
| GPT (ChatGPT)            | Text generation                                |
| T5                       | Text-to-text tasks                             |
| Vision Transformer (ViT) | For image recognition using transformer blocks |








